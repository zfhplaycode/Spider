scrapy框架

- 什么是框架？
	- 集成了很多功能并且具有很强通用性的项目模板。

- 如何学习框架？
	- 专门学习框架封装的各种功能的详细用法。

- 什么是scrapy？
	- 爬虫中封装好的一个明星框架。功能：高性能的持久化存储、异步的数据下载、高性能的数据解析、分布式

scrapy框架的基本使用
	- 环境安装
	- 创建一个工程：scrapy startproject xxxPro
	- cd xxxPro
	- 在spiders子目录中创建一个爬虫文件
		- scrapy genspider spiderName www.xxx.com
	- 执行工程：
		- scrapy crawl spiderName
		- scrapy crawl spiderName --nolog

scrapy数据解析

scrapy持久化存储
	- 基于终端指令：
		- 要求：只可以将parse方法的返回值存储到本地的文本文件中
		- 注意：持久化存储对用的文本文件的类型只可以为：
		- 指令：scrapy crawl xxxPo -o filePath
		- 好处：便捷高效
		- 缺点：局限性强（数据只可以存储在指定后缀的文本文件中）
	- 基于管道：
		- 编码流程：
			- 数据解析
			- 在item中定义相关的属性
			- 将解析的数据封装存储到item类型的对象
			- 将item类型的对象提交给管道进行持久化存储的操作
			- 在管道类的process_item中要将其接收到的item对象存储的数据进行持久化存储操作
			- 在配置文件中开启管道
		- 好处：
			- 通用性强。

	- 面试题：将爬取到的数据一份存储到本地一份存储到数据库，如何实现？
		- 管道文件中一个管道类对应的是将数据存储到一种平台
		- 爬虫文件提交的item只会给管道文件中第一个被执行的管道类接收
		- process_item中的return item表示将item传给下一个即将被执行的管道类

- 基于Spider的全站数据爬取
	- 就是将网站中某板块下的全部页码对应的页面数据进行爬取
	- 需求：爬取校花网中的照片的名称
	- 实现方式：
		- 将所有页面的url添加到start_urls列表（不推荐）
		- 自行手动请求发送（推荐）
			- 手动请求发送：
				- yield scrapy.Request(url,callback):callback专门用于数据解析

- 五大核心组件：https://book.apeland.cn/details/437/

- 请求传参
	- 使用场景：如果爬取解析的数据不在同一张页面中。（深度爬取）
	- 需求：爬取boss的岗位名称和岗位描述

